Resume Project Notes

2025-09-24

Tech stack with broadest appeal to employers:
	Airflow
	dbt
	Postgres
	Superset
	MLflow
	OpenLineage
	GE

Overview of my layered warehouse project:
	Staging --> Silver --> Gold + BI

	1. Staging
	2. Silver
	3. Gold + BI

	Includes:
		dashboard
		lineage
		tests
		optional LLM semantic layer

Roadmap (Ship to GH and post on LinkedIn after each step)

	Day 0: Bootstrap repo --> see bash script for repo config

		GOAL: One-command local stack with empty services; “hello world”
		DAG and README

		0. Install wsl and Docker Desktop for Windows (select "Use the 
		WSL 2 based engine")

		1. Compose all services: Airflow (webserver/scheduler), 
		Postgres (warehouse), MinIO (data lake), Superset, MLflow, 
		Marquez (+ marquez-db), dbt Core (dbt-postgres), 
		Great Expectations, Ollama (local LLM), requests/httpx, 
		bs4 or Scrapy, pydantic/fhir.resources, pandas/pyarrow, 
		scikit-learn --> into Docker file

		2. Add a tiny Airflow DAG with a DummyOperator --> DAG graph

		DELIVERABLES: repo link, docker compose up command,
		Airflow UI/DAG Graph screenshot
	
	Day 1: Ingest synthetic EHR --> bronze (MinIO)
		NOTE: MinIO ==> open-source S3-compatible object storage system,
		an alternative to AWS S3

		GOAL: Pull public/synthetic health data and land raw 
		JSON/CSV/Parquet to MinIO

		1. Pull synthetic data from HAPI FHIR API

		2. Airflow DAG: holland01_ingest_fhir

			- PythonOperator that pages API, validates with 
			pydantic/fhir.resources, writes to 
			s3://bronze/fhir/<resource>/<dt>=YYYY-MM-DD/*.json

			- Respect rate limits; idempotent writes (partition by 
			date/source)

		3. Add Great Expectations (GE) suite for row count/schema checks
		on raw drop (and/or use SQL queries for practice?)

		DELIVERABLES: DAG Graph View, raw object layout in MinIO, 
		GE validation result

	Day 2: Normalize bronze --> Parquet

		GOAL: Make analysis-ready files while keeping raw mutable

		1. Airflow DAG: holland02_normalize
			- Read raw JSON --> flatten into typed Parquet with pyarrow/
			pandas

			- Partition by date/resource; basic data cleaning

			- GE checks on critical columns (IDs, timestamps, controlled
			vocabularies)
		
		DELIVERABLES: Before/after schema diff screenshot; Parquet 
		sample stats (row counts)

	Day 3: Stand up warehouse + dbt staging (Sliver v0)

		GOAL: Queryable SQL layer many employers know

		1. Postgres (in Compose)

		2. dbt Core: set adapter (dbt-postgres), create staging models
		that lightly cast/rename and key your entities

		3. Add dbt tests (not-null, unique) and docs generation; publish
		docs on GitHub Pages from CI

		DELIVERABLES: dbt DAG (model graph) screenshot, link to hosted 
		dbt docs

		Day 4: Silver models (cleaned, joined) + Airflow orchestration

Goal: Model business-ready tables and orchestrate dbt from Airflow.

Build silver models (e.g., encounters, patients, labs) with SCDs/snapshots where useful.

Airflow DAG: dbt_transform

BashOperator (or DockerOperator) to run dbt build --select staging+.

Fail pipeline on dbt test failure.

Emit lineage with openlineage-airflow (configure env vars to Marquez).

Deliverables: Marquez lineage graph screenshot (tables + tasks), failing-test demo (optional).

5) Gold + BI layer + Superset dashboard (shippable)

Goal: Curated marts and a dashboard that auto-refreshes.

gold: star schemas or narrow marts (e.g., fact_encounter, dim_patient, fact_lab_result).

bi: thin, dashboard-oriented tables/materialized views (e.g., daily KPIs).

Superset:

Connect to Postgres; create 2–3 charts + a dashboard.

Save a dataset pointing at your bi tables.

Airflow DAG: bi_refresh

Refresh materialized views, warm caches, export PNGs/CSVs to MinIO (for your LinkedIn posts).

Deliverables: Dashboard screenshot, “BI layer” DDL snippet in README.

6) Data quality gates (GE) + alerts (shippable)

Goal: Professional “stop the line” behavior.

Add Great Expectations validations on silver/gold/bi; store results to MinIO.

Configure Airflow task failure on expectation failure; optional Slack/email alert (local SMTP container works).

Deliverables: Screenshot of failed task + GE validation result page.

7) ML feature table + MLflow (shippable)

Goal: Show ML lifecycle on top of the warehouse.

Create a feature table in gold (e.g., patient-level 30-day readmission features or LOS regression features).

Train a simple scikit-learn model; log params/metrics/artifacts to MLflow (artifact store = MinIO).

Airflow DAG: ml_feature_build → ml_train → ml_evaluate

Store predictions back to Postgres in a bi table for dashboarding.

Deliverables: MLflow UI screenshot (runs/metrics), ROC or feature importance plot.

8) Semantic layer + LLM reporting (optional, shippable)

Goal: Natural-language metrics & automated report text.

Pick one:

Cube (OSS) as a semantic layer over Postgres (define measures/dimensions).

Use Cube’s API to serve metrics to your app/task.

Or a thin, open-source “semantic layer”: a metrics.yml (dbt-style) + a small Python service that maps metric names → SQL.

LLM integration (free-friendly):

Ollama locally for small models (no API costs) to generate narrative summaries from metric values/tables.

Airflow task: query BI metrics → format prompt → generate summary → save Markdown/HTML to MinIO (and commit a copy to the repo if you like).

Keep prompts + outputs versioned to highlight evalability.

Deliverables: Sample “automated weekly report” text file and the metric SQL it came from.

9) CI/CD + IaC polish (shippable)

Goal: Make it “team-ready”.

GitHub Actions:

Lint (ruff, black, isort) + unit tests (pytest).

Build & test dbt (dbt deps, dbt build against a temp Postgres service).

Build docs and publish to GitHub Pages.

Optionally run a compose job to sanity check Airflow DAG parse (airflow dags list).

IaC:

Treat docker-compose.yml as IaC; parameterize with .env.

Add Makefile targets: make up, make down, make nuke, make seed, make screenshots.

(Optional) Terraform module for Neon/Supabase Postgres if you want cloud infra as code in infra/cloud/.

Deliverables: Passing build badge, Pages link to dbt docs, scripted screenshots committed to docs/.

10) Storytelling & posting cadence

Every milestone above is shippable on its own. After each step:

Capture Airflow DAG Graph and/or Marquez lineage.

Include a quick chart (row counts, nulls %, KPI trend).

Post with a short write-up: “what I added, why, and how it fails safely.”

	